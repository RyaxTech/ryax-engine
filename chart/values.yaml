global:
  # -- Override the container registry globaly. Useful to use development using registry.ryax.org/dev or for airgapped env
  imageRegistry: 
  imagePullSecrets: []

  nodeSelector: {}
  # -- Leave empty to use the default storage class
  defaultStorageClass: ""

  # -- Ryax specific configuration
  ryax:
    # -- Global log level, can be overriden locally
    logLevel: warning
    # -- Namespace where user's actions are deployed
    userNamespace: "ryaxns-execs"

  monitoring:
    enabled: true
    # -- TODO: use a tpl in sub chart to inject release name
    otlpEndpoint: "ryax-tempo:4317"

  tls:
    enabled: false
    # -- development or production
    environment: 
    # -- must be a valid FQDN like "local.ryax.io", leave empty for local install
    hostname: ""
    
  # -- Needed by bitnami to avoid https://github.com/bitnami/charts/issues/30850
  security:
    allowInsecureImages: true

  # Be aware that the folowing values are shared amoung the services,
  # but they are defined as static values to avoid unnecessary configuration overhead:
  # 
  # datastoreSecret: ryax-datastore-secret
  # internalRegistry: ryax-registry:5000
  # actionRegistrySecret: ryax-registry-creds-secret
  # brokerSecret: ryax-broker-secret
  # authorizationUrl: ryax-authorization:8080
  # filestoreName: ryax-filestore
  # filestoreSecret: ryax-minio-secret
  # internalRegistry: 127.0.0.1:30012
  # jwtSecret: api-jwt-secret-key

# -- All common configuration and secrets are created here
certManager:
  enabled: false

registry:
  credentials:
    enabled: true
    pullSecretName: ryax-registry-creds-secret
  ingress:
    enabled: false
  persistence:
    enabled: true
    pvcSize: 20Gi
  priorityClass: backbone

datastore:
  pvcSize: 2Gi
  priorityClass: backbone
  
front:
  enabled: true

action-builder:
  priorityClass: "microservices"
  # -- Only necessary if registry.credentials.enabled=true
  actionRegistrySecret: ryax-registry-creds-secret
  persitence:
    enabled: true
  nix:
    storeSize: 100Gi

runner:
  priorityClass: microservices

studio:
  priorityClass: microservices

repository:
  priorityClass: microservices

worker:
  config:
    site:
      name: local
      type: KUBERNETES
      spec:
        namespace: "{{ .Values.global.ryax.userNamespace }}"
        # -- WARNING this is the worker config and the node pool resources and selector have to be set according to your infrastructure
        nodePools:
        - name: default
          cpu: 4
          memory: 8G
          selector:
            kubernetes.io/os: linux
  # -- Only necessary if registry.credentials.enabled=true
  actionRegistrySecret: ryax-registry-creds-secret
  priorityClass: microservices
  promtail:
    serviceMonitor:
      enabled: true
  loki:
    monitoring:
      serviceMonitor:
        enabled: true
  postgresql:
    metrics:
      enabled: true
      serviceMonitor:
        enabled: true
  # -- install intelliscale in a separated helm chart instead
  intelliscale:
    enabled: true
    priorityClass: microservices

repository:
  priorityClass: microservices

minio:
  image:
    repository: bitnamilegacy/minio
  auth:
    existingSecret: ryax-minio-secret
  commonLabels:
    ryax.tech/resource-name: minio
  containerSecurityContext:
    runAsUser: 1200
  metrics:
    enabled: true
  mode: standalone
  persistence:
    enabled: true
    size: 20Gi
  podSecurityContext:
    fsGroup: 1200
  priorityClassName: backbone
  resources:
    limits:
      memory: 1000Mi
    requests:
      cpu: 100m
      memory: 1000Mi
  serviceAccount:
    create: false
  volumePermissions:
    enabled: false

rabbitmq:
  image:
    repository: bitnamilegacy/rabbitmq
  auth:
    existingErlangSecret: ryax-broker-cookie
    existingPasswordSecret: ryax-broker-secret
    resources:
      limits:
        memory: 1000Mi
      requests:
        cpu: 100m
        memory: 500Mi
    tls:
      autoGenerated: false
      enabled: false
    username: ryaxmq
  clustering:
    enabled: false
  extraPlugins: ""
  fullnameOverride: ryax-broker
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
  persistence:
    enabled: true
    size: 1Gi
    # storageClass: local-path
  plugins: rabbitmq_management
  priorityClassName: backbone
  rbac:
    create: false
  serviceAccount:
    create: false
  ulimitNofiles: ""

# -- Configuration for kube-prometheus-chart
kube-prometheus-stack:
  enabled: true
  crds:
    enabled: true
    upgradeJob:
      enabled: true
      forceConflicts: true
  prometheusOperator:
    # -- Node selector for the prometheus 
    # nodeSelector: 
    priorityClassName: monitoring
        
  prometheus:
    prometheusSpec:
      priorityClassName: monitoring
      serviceMonitorSelectorNilUsesHelmValues: false
      additionalScrapeConfigs:
      - job_name: 'kubernetes-gpu-pod'
        scrape_interval: 5s
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
            action: keep
            regex: nvidia-dcgm-exporter
          - source_labels: [__meta_kubernetes_namespace]
            action: keep
            regex: kube-system
          - source_labels: [__meta_kubernetes_pod_container_port_number]
            action: keep
            regex: '9400'
          - source_labels: [__meta_kubernetes_pod_ip, __meta_kubernetes_pod_container_port_number]
            action: replace
            target_label: __address__
            separator: ':'
      # -- inject more labels here like
      # hostedOn: mycloud.com
      # instanceType: production
      externalLabels:
        ryax-version: "{{ .Chart.Version }}"
        cluster: "{{ .Values.global.tls.hostname }}"
      
      # -- Remote write is disabled by default
      # remoteWrite:
      #   url: https://metrics.ryax.org/insert/0/prometheus
    storage:
      volumeClaimTemplate:
        spec:
          # -- Select a storage class for prometheus metrics storage
          # storageClassName: "{{ .Values.global.defaultStorageClass }}"
          resources:
            requests:
              storage: 10Gi
              
  additionalPrometheusRulesMap:
    resource-usage:
      groups:
        - name: resource-usage
          rules:
            - alert: RyaxContainerCpuUsage
              expr: (sum(rate(container_cpu_usage_seconds_total{container=~"ryax-.*"}[15m])) BY (instance, name) * 100) > 95
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: |
                  {{ `"Container CPU usage (instance {{ $labels.instance }})"` }}
                description: |
                  {{ `"Container CPU usage is above 95% for 15 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"` }}
                dashboards: "{{ .Values.dashboardUrl }}/6581e46e4e5c7ba40a07646395ef7b23"
            - alert: RyaxContainerVolumeUsage
              expr: (1 - (sum(container_fs_inodes_free{container=~"ryax-.*"}) BY (instance) / sum(container_fs_inodes_total{container=~"ryax-.*"}) BY (instance)) * 100) > 80
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: |
                  {{ `"Container Volume usage (instance {{ $labels.instance }})"` }}
                description: |
                  {{ `"Container Volume usage is above 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"` }}
                # TODO Create a dashborad for volume usage
            - alert: RyaxContainerVolumeIoUsage
              expr: (sum(container_fs_io_current{container=~"ryax-.*"}) BY (instance, name) * 100) > 80
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: |
                  {{ `"Container Volume IO usage (instance {{ $labels.instance }})"` }}
                description: |
                  {{ `"Container Volume IO usage is above 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"` }}
                # TODO Create a dashborad for volume IO


    meta-monitoring:
      groups:
        - name: meta-monitoring
          rules:
            - alert: InstanceDown
              # Condition for alerting
              expr: up == 0
              for: 5m
              # Annotation - additional informational labels to store more information
              annotations:
                summary: |
                  {{ `'Instance {{ $labels.instance }} down'` }}
                description: |
                  {{ `'{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minute.'` }}
                dashboards: "{{ .Values.dashboardUrl }}/HKcS6KdGk"
              # Labels - additional labels to be attached to the alert
              labels:
                severity: critical
    traefik:
      groups:
        - name: traefik
          rules:
            - alert: TraefikBackendDown
              expr: count(traefik_backend_server_up) by (backend) == 0
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: |
                  {{ `"Traefik backend down (instance {{ $labels.instance }})"` }}
                description: |
                  {{ `"All Traefik backends are down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"` }}
                dashboards: "{{ .Values.dashboardUrl }}/YLMncaOGz"

            - alert: TraefikHighHttp4xxErrorRateBackend
              expr: sum(rate(traefik_backend_requests_total{code=~"4.*"}[3m])) by (backend) / sum(rate(traefik_backend_requests_total[3m])) by (backend) * 100 > 5
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: |
                  {{ `"Traefik high HTTP 4xx error rate backend (instance {{ $labels.instance }})"` }}
                description: |
                  {{ `"Traefik backend 4xx error rate is above 5%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"` }}
                dashboards: "{{ .Values.dashboardUrl }}/YLMncaOGz"

            - alert: TraefikHighHttp5xxErrorRateBackend
              expr: sum(rate(traefik_backend_requests_total{code=~"5.*"}[3m])) by (backend) / sum(rate(traefik_backend_requests_total[3m])) by (backend) * 100 > 5
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: |
                  {{ `"Traefik high HTTP 5xx error rate backend (instance {{ $labels.instance }})"` }}
                description: |
                  {{ `"Traefik backend 5xx error rate is above 5%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"` }}
                dashboards: "{{ .Values.dashboardUrl }}/YLMncaOGz"
            - alert: TraefikHighAvgResponseTime
              expr: sum(traefik_entrypoint_request_duration_seconds_sum) / sum(traefik_entrypoint_requests_total) * 1000 > 500
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: |
                  {{ `"Traefik high average response time (instance {{ $labels.instance }})"` }}
                description: |
                  {{ `"Traefik average response time is higher then 500ms\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"` }}
                dashboards: "{{ .Values.dashboardUrl }}/YLMncaOGz"
    errors:
      groups:
        - name: errors
          rules:
            - alert: RyaxErrorsInLogs
              expr: sum(increase(promtail_custom_ryax_internal_error_in_logs_sum[1m])) > 2
              for: 2m
              labels:
                severity: warning
              annotations:
                summary: |
                  {{ `"Error in Ryax internal servies (container {{ $labels.container }})"` }}
                description: |
                  {{ `"Ryax internal services logs issues more then 2 errors in 1 minute\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"` }}
                dashboards: "{{ .Values.dashboardUrl }}/2D4_jbdGz"

  alertmanager:
    enabled: false

  kubeProxy:
    service:
      selector:
        # -- Needed on AKS to properly select the pod and avoid KubeProxyDown alerts
        component: kube-proxy
  
  # -- Configuration for grafana component
  grafana:
    enabled: true

    grafana.ini:
      users:
        allow_sign_up: false
        allow_org_create: false
      auth.anonymous:
        enabled: false

    ingress:
      enabled: false
          
    persistence:
      enabled: true
      size: 1Gi

    plugins:
      - grafana-piechart-panel
      - grafana-clock-panel
      - vonage-status-panel

    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
      datasources:
        enabled: true
        label: grafana_datasource

    admin:
      # -- This secret is created by common-resources
      existingSecret: grafana-credentials
      userKey: admin-user
      passwordKey: admin-password

tempo:
  enabled: true
  priorityClassName: monitoring
  persistence:
    enabled: true
    size: "10Gi"

traefik:
  deployment:
    enabled: true
  # -- set this to force Traefik on a node pool
  nodeSelector: 
  priorityClassName: backbone
  metrics:
    prometheus:
      # Avoid errors when runnning helm template
      disableAPICheck: true
      serviceMonitor:
        enabled: true
